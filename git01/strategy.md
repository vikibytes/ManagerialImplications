Public: 166th, Private: 66th

R, 2 models averaged: xgboost on all stores, glmnet per store with alpha=1 (lasso regularization), both on log(Sales), with 0.985 correction

Crossvalidation as described at the end of https://www.otexts.org/fpp/2/5 used for selecting glmnet lambda (modified cv.glmnet).

Feature engineering:

 - 5 fourier terms generated by forecast::fourier with frequency=365;
 - days and log(days) since beginning of train set to capture trends;
 - exponential growth before / decay after events like starting promo, state holidays of different length;
 - binary features of different length which took value 1 several days before/after events like start/end of promo, promo2, state holidays, refurbishments;
 - binary features like ClosesTomorrow, WasClosedYesterday, WasClosedOnSunday;
 - day of week, day of month, month number, year as categorical for xbgoost and n-1 binary features for glmnet (described at https://www.otexts.org/fpp/5/2 ).

For some stores with large error on CV dropped data before manually selected (by staring at Sales time series graphs) changepoints.

Dropped stores missing in test set for xgb train set.

Dropped outliers in train set for glmnet. Outliers selected by > 2.5 * median absolute residual from lm trained on small set of features per store.

Initially I used 10 CV folds with 6 weeks length cut from the end of train set with 2 weeks step (~4.5 months total) but then found that closest to 2014 folds produce large errors for stores with missing in 2014 data. Then switched to 15 folds with 3 days step to not get too close to 2014 which improved predictions for those stores.

0.985 correction was insignificant on CV (effect was less than rmspe sd from different folds) but helped on LB both private and public.

Pairwise feature combinations had positive effect for glmnet on CV but didn't work on LB.

arima and tbats were worse than glmnet on the same features, didn't use them.
